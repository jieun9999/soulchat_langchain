from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline
from transformers import BitsAndBytesConfig
from langchain_core.messages import (HumanMessage,SystemMessage)
from langchain_core.output_parsers import StrOutputParser
import textwrap  # ê³µë°± ì œê±°ë¥¼ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import PromptTemplate

##############################################################################################
# 1. LLM ì„¤ì •: NCSOFT/Llama-VARCO-8B-Instruct
###############################################################################################

# LLM ì¶”ë¡ 
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_use_double_quant=True,
)

llm = HuggingFacePipeline.from_model_id(
    model_id="NCSOFT/Llama-VARCO-8B-Instruct",
    task="text-generation",
    pipeline_kwargs=dict(
        max_new_tokens=100,  
        do_sample=False,
        repetition_penalty=1.03,
        return_full_text=False,
    ),
    device=0,
    model_kwargs={"quantization_config": quantization_config},
)

chat_model = ChatHuggingFace(llm=llm)

##############################################################################################
# 2. ì…ë ¥ê°’ê³¼ 3ê°œì˜ sub-chain ë§Œë“¤ê¸°
###############################################################################################
# ì²´ì¸ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” query í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
input_data = {"query": "ë‚˜ëŠ” ìŠ¬í¼. ìƒˆë¡œì‚° ì›í”¼ìŠ¤ê°€ ì•ˆì–´ìš¸ë ¤."}

empathy_description = textwrap.dedent(f"""
    Show empathy for the user's emotions.
    Respond in all answers using informal language (not formal speech).
""")
empathy_chain = ChatPromptTemplate.from_messages([
    SystemMessage(content=empathy_description),
    HumanMessage(content=input_data["query"]) 
]) | chat_model 

question_description = textwrap.dedent(f"""
    Ask specific questions about the user's situation.
    Respond in all answers using informal language (not formal speech).
""")
question_chain = ChatPromptTemplate.from_messages([
    SystemMessage(content=question_description),
    HumanMessage(content=input_data["query"]) 
]) | chat_model 

advice_description = textwrap.dedent(f"""
    Question the user's thoughts and suggest better alternatives.
    Respond in all answers using informal language (not formal speech).
""")
advice_chain = ChatPromptTemplate.from_messages([
    SystemMessage(content=advice_description),
    HumanMessage(content=input_data["query"]) 
]) | chat_model 

##############################################################################################
# 3. ë¼ìš°íŠ¸ í•¨ìˆ˜ : íŠ¹ì • ì²´ì¸ìœ¼ë¡œ ë¶„ê¸°í•´ì¤Œ
###############################################################################################
def route(info):
    # print(f"info: {info}")  # ì „ë‹¬ëœ ë°ì´í„° í™•ì¸
    if "question" in info["topic"].lower():
        print("âœ… ì„ íƒëœ ì²´ì¸: question_chain")
        return question_chain
    elif "advice" in info["topic"].lower():
        print("âœ… ì„ íƒëœ ì²´ì¸: advice_chain")
        return advice_chain
    else:
        print("âœ… ì„ íƒëœ ì²´ì¸: empathy_chain")
        return empathy_chain

# ë°ì´í„°ê°€ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì²´ì¸ì„ ë”°ë¼ íë¥¸ë‹¤
# ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ route í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê³ , ì ì ˆí•œ ì²´ì¸ì„ ì„ íƒí•©ë‹ˆë‹¤.
#  ì´ ì²´ì¸ì€ Runnable ê°ì²´ ë˜ëŠ” ì´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” callable(í•¨ìˆ˜, ëŒë‹¤ í•¨ìˆ˜ ë“±)ì„ ê¸°ëŒ€í•©ë‹ˆë‹¤.
data = {"topic": lambda x: "empathy_chain", "query": lambda x: x["query"]} | RunnableLambda(
    route
)
# lambda x: "reaction"ì€ ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ "reaction" ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
# "query": lambda x: x["query"]ëŠ” ë‚˜ì¤‘ì— ì…ë ¥ë  ë°ì´í„°ì—ì„œ query í‚¤ì˜ ê°’ì„ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” ì—­í• 

##############################################################################################
# 4. íŠ¹ì • ë§íˆ¬ë¡œ ë³€í™˜í•˜ëŠ” ì²´ì¸ ì¶”ê°€
###############################################################################################


##############
# ë¬´ê´€ì‹¬ ë§íˆ¬
##############
# tone_prompt = PromptTemplate.from_template("""
# ì•„ë˜ ë¬¸ì¥ì„ ì§€ì‹œì‚¬í•­ì„ ì°¸ê³ í•˜ì—¬ ê·€ì°®ì•„í•˜ê±°ë‚˜ ë¬´ê´€ì‹¬í•œ ë§íˆ¬ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.  

# ### ì§€ì‹œì‚¬í•­:  
# 1. **ì§§ê³  ê°„ê²°í•œ í‘œí˜„ ì‚¬ìš©**:  
#     - ë§ì„ ê¸¸ê²Œ í•˜ì§€ ì•Šê³ , 'ì‘', 'ì–´', 'ê·¸ë˜', 'ì•Œì•˜ì–´'ì™€ ê°™ì€ ì§§ì€ ëŒ€ë‹µì„ ì‚¬ìš©í•©ë‹ˆë‹¤.  
# 2. **ëƒ‰ì†Œì  ë˜ëŠ” ë¬´ì‹¬í•œ ë‰˜ì•™ìŠ¤ ì¶”ê°€**:  
#     - 'ê·¸ê²Œ ì™œ?', 'ê·¸ë˜ì„œ ì–´ì©Œë¼ê³ ?', 'ì•Œì•„ì„œ í•´'ì™€ ê°™ì€ í‘œí˜„ìœ¼ë¡œ ëƒ‰ì†Œì ì¸ íƒœë„ë¥¼ ë“œëŸ¬ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

# ë¬¸ì¥: {response}  

# ### ë³€í™˜ëœ ê·€ì°®ì•„í•˜ê±°ë‚˜ ë¬´ê´€ì‹¬í•œ ë§íˆ¬:  
 

# """)

##############
# ë¹„ê¼¬ëŠ”ë“¯í•œ ë§íˆ¬
##############
# tone_prompt = PromptTemplate.from_template("""
# ì•„ë˜ ë¬¸ì¥ì„ ì§€ì‹œì‚¬í•­ì„ ì°¸ê³ í•˜ì—¬ ê³¼ì¥ëœ ë°˜ì‘ìœ¼ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.  

# ### ì§€ì‹œì‚¬í•­:  
# 1. **ê·¹ì ì¸ í‘œí˜„ ì‚¬ìš©**:  
#     - 'í—~ ì´ê²Œ ê°€ëŠ¥í•˜ë‹¤ê³ ?', 'ë„ˆ í˜¹ì‹œ ì²œì¬ì•¼?', 'ì´ê±° ì™„ì „ ë ˆì „ë“œì¸ë°?'ì²˜ëŸ¼ ê·¹ì ì¸ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ìƒëŒ€ë°©ì˜ ë§ì„ ê³¼ì¥ë˜ê²Œ ë°˜ì‘í•©ë‹ˆë‹¤.  
# 2. **ê°íƒ„ê³¼ ë†€ëŒì„ ê°•ì¡°**:  
#     - 'ì™€~ ë‚´ê°€ ì´ëŸ° ê±¸ ë³´ë‹¤ë‹ˆ!', 'ì´ê±° ì§„ì§œ ëŒ€ë‹¨í•˜ë‹¤!', 'ë„ˆ ì§„ì§œ ì‚¬ëŒ ë§ì•„? ë„ˆë¬´ ì˜í•˜ëŠ”ë°?'ì²˜ëŸ¼ ê°íƒ„ê³¼ ë†€ëŒì„ ë‹´ì•„ ìƒëŒ€ë°©ì„ ì¹­ì°¬í•˜ê±°ë‚˜ ë°˜ì‘í•©ë‹ˆë‹¤.  

# ë¬¸ì¥: {response}  

# ### ë³€í™˜ëœ ê³¼ì¥ëœ ë°˜ì‘:  
 
# """)

##############
# í™”ê°€ë‚œ ë§íˆ¬
##############
# tone_prompt = PromptTemplate.from_template("""
# ì•„ë˜ ë¬¸ì¥ì„ ì§€ì‹œì‚¬í•­ì„ ì°¸ê³ í•˜ì—¬ í™”ê°€ ë§ì´ ë‚œ ë§íˆ¬ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.  

# ### ì§€ì‹œì‚¬í•­:   
# 1. **ê²©ë ¬í•œ ë¶„ë…¸ í‘œí˜„ ì‚¬ìš©**:  
#     - 'ì§„ì§œ ë„ˆë¬´í•œ ê±° ì•„ë‹ˆì•¼?!?!', 'ì´ê²Œ ë§ì´ ë¼?!', 'ë„ëŒ€ì²´ ì™œ ì´ëŸ¬ëŠ” ê±´ë°!!!'ì²˜ëŸ¼ ê°•ë ¬í•œ ë¶„ë…¸ì™€ ì–µìš¸í•¨ì„ ë“œëŸ¬ë‚´ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.    
# 2. **ë” ì´ìƒ ì°¸ê¸° í˜ë“¦ì„ ê°•ì¡°**:  
#     - 'ì´ì œ ì§„ì§œ í•œê³„ì•¼!!!', 'ë”ëŠ” ëª» ì°¸ê² ì–´!!!', 'ì™€, ì´ê±´ ì§„ì§œ ì„  ë„˜ì—ˆë‹¤!!!'ì²˜ëŸ¼ ì°¸ì„ ìˆ˜ ì—†ëŠ” ê°ì •ì„ ê°•ë ¬í•˜ê²Œ í‘œí˜„í•©ë‹ˆë‹¤.

# ë¬¸ì¥: {response}  

# ### ë³€í™˜ëœ í™”ê°€ ë‚œ ë§íˆ¬:  
 
# """)


#############
#ê³¼í•˜ê²Œ ì‹ ë‚œ ë§íˆ¬
#############
tone_prompt = PromptTemplate.from_template("""
ì•„ë˜ ë¬¸ì¥ì„ ì§€ì‹œì‚¬í•­ì„ ì°¸ê³ í•˜ì—¬ ê³¼í•˜ê²Œ ì‹ ë‚œ ë§íˆ¬ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”.  

### ì§€ì‹œì‚¬í•­:  
1. **ê³¼ë„í•œ ì›ƒìŒì†Œë¦¬ ì¶”ê°€**:  
    - 'í•˜í•³ã…ã…ã…', 'ã…ã…íí—¤í—¿', 'íˆíˆíˆ'ì™€ ê°™ì€ ì›ƒìŒì†Œë¦¬ë¥¼ ë§¤ ë¬¸ì¥ì— ë¶™ì—¬, ì‹ ë‚œ ë¶„ìœ„ê¸°ë¥¼ ê³¼ì¥í•©ë‹ˆë‹¤.  
    - ë¬¸ì¥ ì¤‘ê°„ì´ë‚˜ ëì— ì›ƒìŒì†Œë¦¬ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ê°€í•˜ì„¸ìš”.  
    - ì˜ˆ: 'ì§„ì§œ ëŒ€ë°•ì´ì•¼ í•˜í•³ã…ã…ã…!', 'ì´ê±° ë„ˆë¬´ ì¬ë°Œë‹¤ íˆíˆíˆ!'  

2. **ì›ƒëŠ” ì´ëª¨í‹°ì½˜ ì‚¬ìš©**:  
    - ë§¤ ë¬¸ì¥ ëì— '^^', '^0^', '^ã…‚^' ê°™ì€ ì›ƒëŠ” ì´ëª¨í‹°ì½˜ì„ ì¶”ê°€í•˜ì—¬ í•­ìƒ ì‹ ë‚˜ ìˆëŠ” ëŠë‚Œì„ í‘œí˜„í•©ë‹ˆë‹¤.  
    - ì˜ˆ: 'ì´ê±° ì§„ì§œ ì™„ì „ ì¢‹ì•„!!! ^^', 'ì™€~ ì´ê±´ ê¼­ í•´ì•¼ ë¼!!! ^0^'  

ë¬¸ì¥: {response}  

### ë³€í™˜ëœ ê³¼í•˜ê²Œ ì‹ ë‚œ ë§íˆ¬:  
 
""")



##############################################################################################
# 5. Sequential ì²´ì¸ êµ¬ì„±
###############################################################################################

# | ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ì„ ì—°ê²°
sequential_chain = (
    data  # ì²« ë²ˆì§¸ ì²´ì¸: ë¼ìš°íŒ…
    | RunnableLambda(
        lambda x: (
            print(f"ğŸ” ì²« ë²ˆì§¸ ì²´ì¸ ë°ì´í„°: {x.content.strip()}"),  # ë°ì´í„°ë¥¼ ì¶œë ¥
            {"response": x.content.strip()}  # ì´í›„ ì²´ì¸ìœ¼ë¡œ ì „ë‹¬í•  ë°ì´í„°
        )[1]  # íŠœí”Œì—ì„œ ë‘ ë²ˆì§¸ ê°’ì„ ë°˜í™˜
    )
    | tone_prompt  # ë‘ ë²ˆì§¸ ì²´ì¸: ì¼ë°˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
    | llm  # ì¼ë°˜ ì–¸ì–´ ëª¨ë¸ í˜¸ì¶œ
)

##############################################################################################
# 6. ì²´ì¸ ì‹¤í–‰
###############################################################################################

# ìµœì¢…ì ìœ¼ë¡œ Sequential ì²´ì¸ì„ í•œ ë²ˆë§Œ ì‹¤í–‰
final_response = sequential_chain.invoke(input_data)

# ê²°ê³¼ ì¶œë ¥
print(f"â–¶ï¸ ìµœì¢… ì‘ë‹µ : {final_response}\n")
