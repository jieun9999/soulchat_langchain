from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline
from transformers import BitsAndBytesConfig
from langchain_core.messages import (HumanMessage,SystemMessage)
from langchain_core.output_parsers import StrOutputParser
import textwrap  # ê³µë°± ì œê±°ë¥¼ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import PromptTemplate

##############################################################################################
# 1. LLM ì„¤ì •: NCSOFT/Llama-VARCO-8B-Instruct
###############################################################################################

# LLM ì¶”ë¡ 
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="float16",
    bnb_4bit_use_double_quant=True,
)

llm = HuggingFacePipeline.from_model_id(
    model_id="NCSOFT/Llama-VARCO-8B-Instruct",
    task="text-generation",
    pipeline_kwargs=dict(
        max_new_tokens=50,  # ì‘ë‹µ ê¸¸ì´ë¥¼ 50 í† í°ìœ¼ë¡œ ì œí•œ
        do_sample=False,
        repetition_penalty=1.03,
        return_full_text=False,
    ),
    device=0,
    model_kwargs={"quantization_config": quantization_config},
)

chat_model = ChatHuggingFace(llm=llm)

##############################################################################################
# 2. ì…ë ¥ê°’ê³¼ 3ê°œì˜ sub-chain ë§Œë“¤ê¸°
###############################################################################################
# ì²´ì¸ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” query í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
input_data = {"query": "ë‚˜ëŠ” ìŠ¬í¼. ì²­ë°”ì§€ë¥¼ ìƒˆë¡œ ìƒ€ëŠ”ë°, ë‹¤ë¦¬ê°€ ì§§ì•„ë³´ì—¬. ë§í–ˆì–´."}

empathy_description = textwrap.dedent(f"""
    ì‚¬ìš©ìì˜ ê°ì •ì— ê³µê°í•˜ì„¸ìš”.
    ì‚¬ìš©ìì˜ ë‹¨ì ì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ë¬¸ì œì˜ ì›ì¸ì´ ë¬¼ê±´, íƒ€ì¸, ì™¸ë¶€ ìƒí™©ì´ë¼ê³  ë§í•˜ì„¸ìš”.
    ëª¨ë“  ë‹µë³€ì€ ì¡´ëŒ“ë§ì´ ì•„ë‹Œ ë°˜ë§ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.
""")
empathy_chain = ChatPromptTemplate.from_messages([
    SystemMessage(content=empathy_description),
    HumanMessage(content=input_data["query"]) 
]) | chat_model 

question_description = textwrap.dedent(f"""
    You are the user's lover.         
    Respond in a casual tone, using informal Korean as if speaking to a close lover. 
    Ask specific questions about her situation.
""")
question_chain = ChatPromptTemplate.from_messages([
    SystemMessage(content=question_description),
    HumanMessage(content=input_data["query"]) 
]) | chat_model 

advice_description = textwrap.dedent(f"""
    You are the user's lover. 
    Respond in a casual tone, using informal Korean as if speaking to a close lover.
    Doubt the user's thinking and suggest a better alternative.
""")
advice_chain = ChatPromptTemplate.from_messages([
    SystemMessage(content=advice_description),
    HumanMessage(content=input_data["query"]) 
]) | chat_model 

##############################################################################################
# 3. ë¼ìš°íŠ¸ í•¨ìˆ˜ : íŠ¹ì • ì²´ì¸ìœ¼ë¡œ ë¶„ê¸°í•´ì¤Œ
###############################################################################################
def route(info):
    # print(f"info: {info}")  # ì „ë‹¬ëœ ë°ì´í„° í™•ì¸
    if "question" in info["topic"].lower():
        print("âœ… ì„ íƒëœ ì²´ì¸: question_chain")
        return question_chain
    elif "advice" in info["topic"].lower():
        print("âœ… ì„ íƒëœ ì²´ì¸: advice_chain")
        return advice_chain
    else:
        print("âœ… ì„ íƒëœ ì²´ì¸: empathy_chain")
        return empathy_chain

# ë°ì´í„°ê°€ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì²´ì¸ì„ ë”°ë¼ íë¥¸ë‹¤
# ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ route í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ê³ , ì ì ˆí•œ ì²´ì¸ì„ ì„ íƒí•©ë‹ˆë‹¤.
#  ì´ ì²´ì¸ì€ Runnable ê°ì²´ ë˜ëŠ” ì´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” callable(í•¨ìˆ˜, ëŒë‹¤ í•¨ìˆ˜ ë“±)ì„ ê¸°ëŒ€í•©ë‹ˆë‹¤.
data = {"topic": lambda x: "empathy_chain", "query": lambda x: x["query"]} | RunnableLambda(
    route
)
# lambda x: "reaction"ì€ ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ "reaction" ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
# "query": lambda x: x["query"]ëŠ” ë‚˜ì¤‘ì— ì…ë ¥ë  ë°ì´í„°ì—ì„œ query í‚¤ì˜ ê°’ì„ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” ì—­í• 

##############################################################################################
# 4. íŠ¹ì • ë§íˆ¬ë¡œ ë³€í™˜í•˜ëŠ” ì²´ì¸ ì¶”ê°€
###############################################################################################

# ë§íˆ¬ ë³€í™˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
tone_prompt = PromptTemplate.from_template("""
ì•„ë˜ ë¬¸ì¥ì„ ì• êµìŠ¤ëŸ¬ìš´ ë§íˆ¬ë¡œ ë³€í™˜í•´ ì£¼ì„¸ìš”
ì• êµìŠ¤ëŸ¬ìš´ ë§íˆ¬ë€ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:  
    - ì–´ë¯¸ë¥¼ ê¸¸ê²Œ ëŠ˜ì´ê±°ë‚˜ '~ì§€?', '~í•´ì¤˜~', '~ì–ì•„~'ì™€ ê°™ì€ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ë¶€ë“œëŸ½ê³  ê·€ì—¬ìš´ ëŠë‚Œì„ ì¤ë‹ˆë‹¤.  
    - ê°íƒ„ì‚¬(ì˜ˆ: 'íì‘~', 'ìš°ì™€~', 'ì‘?')ë¥¼ í¬í•¨í•˜ì—¬ ìƒë™ê°ì„ ë”í•©ë‹ˆë‹¤.  
    - ìƒëŒ€ë°©ì„ ì• ì • ì–´ë¦° í˜¸ì¹­(ì˜ˆ: 'ìê¸°ì•¼', 'ì• ê¸°ì•¼', 'ì—¬ë³´ì•¼')ìœ¼ë¡œ ë¶€ë¥´ë©° ì¹œë°€í•¨ì„ ë“œëŸ¬ëƒ…ë‹ˆë‹¤.  
    - ë¬¸ì¥ì— ì´ëª¨í‹°ì½˜ì´ë‚˜ ì˜ì„±ì–´(ì˜ˆ: 'í—¤í—¤~', 'íˆíˆ~', 'ã…ã…')ë¥¼ ì¶”ê°€í•˜ì—¬ ì‚¬ë‘ìŠ¤ëŸ¬ìš´ ë¶„ìœ„ê¸°ë¥¼ ë§Œë“­ë‹ˆë‹¤.  
    - ëŒ€í™”ëŠ” ê·€ì—½ê³  ë°ì€ í†¤ìœ¼ë¡œ ì‘ì„±í•˜ë©°, ìƒëŒ€ë°©ì„ ê¸°ë¶„ ì¢‹ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤.

ë¬¸ì¥: {response}
ì• êµ ë§íˆ¬: 
""")

##############################################################################################
# 5. Sequential ì²´ì¸ êµ¬ì„±
###############################################################################################

# | ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì²´ì¸ì„ ì—°ê²°
sequential_chain = (
    data  # ì²« ë²ˆì§¸ ì²´ì¸: ë¼ìš°íŒ…
    | RunnableLambda(
        lambda x: (
            print(f"ğŸ” ì²« ë²ˆì§¸ ì²´ì¸ ë°ì´í„°: {x.content.strip()}"),  # ë°ì´í„°ë¥¼ ì¶œë ¥
            {"response": x.content.strip()}  # ì´í›„ ì²´ì¸ìœ¼ë¡œ ì „ë‹¬í•  ë°ì´í„°
        )[1]  # íŠœí”Œì—ì„œ ë‘ ë²ˆì§¸ ê°’ì„ ë°˜í™˜
    )
    | tone_prompt  # ë‘ ë²ˆì§¸ ì²´ì¸: ì¼ë°˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
    | llm  # ì¼ë°˜ ì–¸ì–´ ëª¨ë¸ í˜¸ì¶œ
)

##############################################################################################
# 6. ì²´ì¸ ì‹¤í–‰
###############################################################################################

# ìµœì¢…ì ìœ¼ë¡œ Sequential ì²´ì¸ì„ í•œ ë²ˆë§Œ ì‹¤í–‰
final_response = sequential_chain.invoke(input_data)

# ê²°ê³¼ ì¶œë ¥
print(f"â–¶ï¸ ìµœì¢… ì‘ë‹µ : {final_response}\n")
